{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A Transformer Implementation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI5Ecd1YabAS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import SGD\n",
        "import torchtext\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext import data\n",
        "from torchtext.data.metrics import bleu_score\n",
        "import spacy\n",
        "from spacy.symbols import ORTH\n",
        "import math\n",
        "import random\n",
        "import tqdm.notebook as tq\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LSTMCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.linear_input = nn.Linear(input_size, 4 * hidden_size)\n",
        "        self.linear_hidden = nn.Linear(hidden_size, 4 * hidden_size)\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        # WRITE YOUR CODE HERE\n",
        "        hx = state[0]\n",
        "        cx = state[1]\n",
        "\n",
        "        out1 = self.linear_hidden(hx) + self.linear_input(x)\n",
        "        chunk_forgetgate, chunk_ingate, chunk_cellgate, chunk_outgate = torch.chunk(out1, chunks=4, dim=1)\n",
        "\n",
        "        fx = torch.sigmoid(chunk_forgetgate)\n",
        "        ix = torch.sigmoid(chunk_ingate)\n",
        "        c_hat_y = torch.tanh(chunk_cellgate)\n",
        "        ox = torch.sigmoid(chunk_outgate)\n",
        "\n",
        "        cy = cx*fx + ix*c_hat_y\n",
        "        hy = torch.tanh(cy) * ox\n",
        "\n",
        "        return hy, (hy, cy)"
      ],
      "metadata": {
        "id": "2N3NlOWUm-VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMLayer(nn.Module):\n",
        "    def __init__(self,*cell_args):\n",
        "        super(LSTMLayer, self).__init__()\n",
        "        self.cell = LSTMCell(*cell_args)\n",
        "\n",
        "    def forward(self, x, state, length_x=None):\n",
        "        # DO NOT MODIFY\n",
        "        # type: (Tensor, Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]\n",
        "        inputs = x.unbind(0)\n",
        "        assert (length_x is None) or torch.all(length_x == length_x.sort(descending=True)[0])\n",
        "        outputs = [] \n",
        "        out_hidden_state = []\n",
        "        out_cell_state = []\n",
        "        for i in range(len(inputs)):\n",
        "            out, state = self.cell(inputs[i] , state)\n",
        "            outputs += [out] \n",
        "            if length_x is not None:\n",
        "                if torch.any(i+1 == length_x):\n",
        "                    out_hidden_state = [state[0][i+1==length_x]] + out_hidden_state\n",
        "                    out_cell_state = [state[1][i+1==length_x]] + out_cell_state\n",
        "        if length_x is not None:\n",
        "            state = (torch.cat(out_hidden_state, dim=0), torch.cat(out_cell_state, dim=0))\n",
        "        return torch.stack(outputs), state "
      ],
      "metadata": {
        "id": "MoP9BGGOqrqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, ninp, nhid, num_layers, dropout):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.layers = []\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        for i in range(num_layers):\n",
        "            if i == 0:\n",
        "                self.layers.append(LSTMLayer(ninp, nhid))\n",
        "            else:\n",
        "                self.layers.append(LSTMLayer(nhid, nhid)) \n",
        "        self.layers = nn.ModuleList(self.layers) \n",
        "\n",
        "    def forward(self, x, states, length_x=None):\n",
        "        # WRITE YOUR CODE HERE\n",
        "\n",
        "        output_states = []\n",
        "        result = x\n",
        "        num_layers = len(self.layers)\n",
        "\n",
        "        for i in range(num_layers):\n",
        "          result, output_state = self.layers[i](result, states[i], length_x = length_x)\n",
        "          if i != num_layers - 1:\n",
        "            result = self.dropout(result)\n",
        "          output_states.append(output_state)\n",
        "        output = result\n",
        "        return output, output_states"
      ],
      "metadata": {
        "id": "NVDuSFwsrNtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        ninp = args.ninp\n",
        "        nhid = args.nhid\n",
        "        nlayers = args.nlayers\n",
        "        dropout = args.dropout\n",
        "        self.embed = nn.Embedding(src_ntoken, ninp, padding_idx=pad_id)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lstm = LSTM(ninp, nhid, nlayers, dropout)\n",
        "        \n",
        "    def forward(self, x, states, length_x=None):\n",
        "        # WRITE YOUR CODE HERE\n",
        "        out1 = self.embed(x)\n",
        "        out1 = self.dropout(out1)\n",
        "        output, context_vectors = self.lstm(out1, states, length_x)\n",
        "        return output, context_vectors"
      ],
      "metadata": {
        "id": "brLUn8sjrRk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.embed = nn.Embedding(trg_ntoken, args.ninp, padding_idx=pad_id)\n",
        "        self.lstm = LSTM(args.ninp, args.nhid, args.nlayers, args.dropout)\n",
        "        self.fc_out = nn.Linear(args.nhid, trg_ntoken)\n",
        "        self.dropout = nn.Dropout(args.dropout)\n",
        "        self.fc_out.weight = self.embed.weight\n",
        "        \n",
        "    def forward(self, x, states):\n",
        "        # WRITE YOUR CODE HERE\n",
        "        out1 = self.embed(x)\n",
        "        out1 = self.dropout(out1)\n",
        "        output, output_states = self.lstm(out1, states)\n",
        "\n",
        "        output = self.fc_out(output)\n",
        "\n",
        "        return output, output_states"
      ],
      "metadata": {
        "id": "xRXkU61brVNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMSeq2Seq(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMSeq2Seq, self).__init__()\n",
        "        self.encoder = LSTMEncoder()\n",
        "        self.decoder = LSTMDecoder()\n",
        "    \n",
        "    def _get_init_states(self, x):\n",
        "        init_states = [\n",
        "            (torch.zeros((x.size(1), args.nhid)).to(x.device),\n",
        "            torch.zeros((x.size(1), args.nhid)).to(x.device))\n",
        "            for _ in range(args.nlayers)\n",
        "        ]\n",
        "        return init_states\n",
        "\n",
        "    def forward(self, x, y, length, max_len=None, teacher_forcing=True):\n",
        "        # WRITE YOUR CODE HERE\n",
        "        init_states = self._get_init_states(x)\n",
        "        _, output_states = self.encoder(x, init_states, length)\n",
        "\n",
        "        dec_input = y[0:1]\n",
        "\n",
        "        if max_len is None:\n",
        "          trg_len = y.size(0)\n",
        "        else:\n",
        "          trg_len = max_len\n",
        "        output_token = []\n",
        "\n",
        "        dec_output, states = self.decoder(dec_input, output_states)\n",
        "        output_token.append(dec_output)\n",
        "\n",
        "        for i in range(1, trg_len - 1):\n",
        "\n",
        "          dec_input = dec_output.argmax(-1)\n",
        "          if teacher_forcing:\n",
        "            dec_input = y[i: i+1]\n",
        "\n",
        "          dec_output, states = self.decoder(dec_input, states)\n",
        "          output_token.append(dec_output)\n",
        "\n",
        "        output = torch.cat(output_token)\n",
        "        return output"
      ],
      "metadata": {
        "id": "_Kf-vzmIrbhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.nhid_enc = args.nhid\n",
        "        self.nhid_dec = args.nhid\n",
        "        self.W1 = nn.Linear(self.nhid_enc, args.nhid_attn)\n",
        "        self.W2 = nn.Linear(self.nhid_dec, args.nhid_attn)\n",
        "        self.W3 = nn.Linear(args.nhid_attn, 1)\n",
        "\n",
        "    def forward(self, x, enc_o, dec_h, length_enc=None):\n",
        "        # WRITE YOUR CODE HERE\n",
        "        first = self.W1(enc_o)\n",
        "        second = self.W2(dec_h)\n",
        "\n",
        "        score = self.W3(torch.tanh(first + second))\n",
        "        score = torch.squeeze(score)\n",
        "        L, B = score.size()\n",
        "\n",
        "        if length_enc is not None:\n",
        "          lol = torch.range(0, L-1).reshape(L, -1).expand(L, B).to(device)\n",
        "          xd = length_enc.expand(L, length_enc.shape[0])\n",
        "          score[lol > xd] = float(\"-inf\")\n",
        "        \n",
        "        score = torch.unsqueeze(score, 2)\n",
        "        out = F.softmax(score, dim=0)\n",
        "        out = out * enc_o\n",
        "        out = torch.sum(out, dim=0, keepdims=True)\n",
        "        out = torch.cat((x, out), dim=-1)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "TWEVoU-lu1hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMAttnDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMAttnDecoder, self).__init__()\n",
        "        self.embed = nn.Embedding(trg_ntoken, args.ninp, padding_idx=pad_id)\n",
        "        self.lstm = LSTM(args.ninp + args.nhid, args.nhid, args.nlayers, args.dropout)\n",
        "        self.fc_out = nn.Linear(args.nhid, trg_ntoken)\n",
        "        self.dropout = nn.Dropout(args.dropout)\n",
        "        self.attn = Attention()\n",
        "        self.fc_out.weight = self.embed.weight\n",
        "        \n",
        "    def forward(self, x, enc_o, states, length_enc=None):\n",
        "        # WRITE YOUR CODE HERE\n",
        "        out = self.embed(x)\n",
        "        out = self.dropout(out)\n",
        "        state, _ = states[-1]\n",
        "\n",
        "        out = self.attn(out, enc_o, state, length_enc)\n",
        "        output, output_states = self.lstm(out, states)\n",
        "        output = self.fc_out(output)\n",
        "        return output, output_states "
      ],
      "metadata": {
        "id": "Sd9IO8mbvDIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMAttnSeq2Seq(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMAttnSeq2Seq, self).__init__()\n",
        "        self.encoder = LSTMEncoder()\n",
        "        self.decoder = LSTMAttnDecoder()\n",
        "    \n",
        "    def _get_init_states(self, x):\n",
        "        init_states = [\n",
        "            (torch.zeros((x.size(1), args.nhid)).to(x.device),\n",
        "            torch.zeros((x.size(1), args.nhid)).to(x.device))\n",
        "            for _ in range(args.nlayers)\n",
        "        ]\n",
        "        return init_states\n",
        "\n",
        "    def forward(self, x, y, length, max_len=None, teacher_forcing=True):\n",
        "        # WRITE YOUR CODE HERE\n",
        "        init_states = self._get_init_states(x)\n",
        "        \n",
        "        enc_output, _ = self.encoder(x, init_states, length)\n",
        "\n",
        "        dec_input = y[0:1]\n",
        "\n",
        "        if max_len is None:\n",
        "          trg_len = y.size(0)\n",
        "        else:\n",
        "          trg_len = max_len\n",
        "        output_token = []\n",
        "\n",
        "        dec_output, states = self.decoder(dec_input, enc_output, init_states, length)\n",
        "        output_token.append(dec_output)\n",
        "\n",
        "        for i in range(1, trg_len - 1):\n",
        "          dec_input = dec_output.argmax(-1)\n",
        "          if teacher_forcing:\n",
        "            dec_input = y[i: i+1]\n",
        "\n",
        "          dec_output, states = self.decoder(dec_input, enc_output, states, length)\n",
        "          output_token.append(dec_output)\n",
        "\n",
        "        output = torch.cat(output_token)\n",
        "        #print(output.shape)\n",
        "        return output"
      ],
      "metadata": {
        "id": "of2fFOzDvwxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 100\n",
        "class MaskedMultiheadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked attention layer with a projection at the end.\n",
        "    \"\"\"\n",
        "    def __init__(self, mask=False):\n",
        "        super(MaskedMultiheadAttention, self).__init__()\n",
        "        assert args.nhid_tran % args.nhead == 0\n",
        "        # mask : whether to use \n",
        "        # key, query, value projections for all heads\n",
        "        self.key = nn.Linear(args.nhid_tran, args.nhid_tran)\n",
        "        self.query = nn.Linear(args.nhid_tran, args.nhid_tran)\n",
        "        self.value = nn.Linear(args.nhid_tran, args.nhid_tran)\n",
        "        # regularization\n",
        "        self.attn_drop = nn.Dropout(args.attn_pdrop)\n",
        "        # output projection\n",
        "        self.proj = nn.Linear(args.nhid_tran, args.nhid_tran)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        if mask:\n",
        "            self.register_buffer(\"mask\", torch.tril(torch.ones(MAX_LEN, MAX_LEN)))\n",
        "        self.nhead = args.nhead\n",
        "        self.d_k = args.nhid_tran // args.nhead\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        # WRITE YOUR CODE HERE\n",
        "        q1 = torch.transpose(self.query(q).reshape(q.size(0), q.size(1), self.nhead, self.d_k), 1, 2)\n",
        "        k1 = torch.transpose(self.key(k).reshape(k.size(0), k.size(1), self.nhead, self.d_k), 1, 2)\n",
        "        v1 = torch.transpose(self.value(v).reshape(v.size(0), v.size(1), self.nhead, self.d_k), 1, 2)\n",
        "\n",
        "        score = torch.matmul(q1, torch.transpose(k1, 2, 3))\n",
        "        score = score/math.sqrt(self.d_k)\n",
        "        B, nhead, T_q, T = score.size()\n",
        "        if hasattr(self, 'mask'):\n",
        "          mask2 = torch.stack([torch.stack([self.mask[:T_q, :T]] * score.size(1))] * score.size(0))\n",
        "          #score[mask2 == 0] = float('-inf')\n",
        "          score[self.mask == 0] = float('-inf')\n",
        "        \n",
        "        if mask is not None:\n",
        "          mask1 = torch.stack([torch.stack([mask] * score.size(1), 1)] * score.size(2), 2)\n",
        "          #score[mask1 == 0] = float('-inf')\n",
        "          score[mask == 0] = float('-inf')\n",
        "\n",
        "        score = F.softmax(score, dim=-1)\n",
        "        score = self.attn_drop(score)\n",
        "        score = torch.matmul(score, v1)\n",
        "\n",
        "        score = torch.transpose(score, 1, 2)\n",
        "        score = score.reshape(score.size(0), score.size(1), -1)\n",
        "        score = self.proj(score)\n",
        "        \n",
        "        return score"
      ],
      "metadata": {
        "id": "CXsy8OfhwONT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerEncLayer, self).__init__()\n",
        "        self.ln1 = nn.LayerNorm(args.nhid_tran)\n",
        "        self.ln2 = nn.LayerNorm(args.nhid_tran)\n",
        "        self.attn = MaskedMultiheadAttention()\n",
        "        self.dropout1 = nn.Dropout(args.resid_pdrop)\n",
        "        self.dropout2 = nn.Dropout(args.resid_pdrop)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(args.nhid_tran, args.nff),\n",
        "            nn.ReLU(), \n",
        "            nn.Linear(args.nff, args.nhid_tran)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # WRITE YOUR CODE HERE\n",
        "        out1 = self.ln1(x)\n",
        "        out = self.attn(out1, out1, out1, mask)\n",
        "        out = self.dropout1(out)\n",
        "        out = out + out1\n",
        "        \n",
        "        out2 = self.ln2(out)\n",
        "        out = self.ff(out2)\n",
        "        out = self.dropout2(out)\n",
        "        out = out + out2\n",
        "        \n",
        "        return out\n"
      ],
      "metadata": {
        "id": "NqwbV6eewbAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerDecLayer, self).__init__()\n",
        "        self.ln1 = nn.LayerNorm(args.nhid_tran)\n",
        "        self.ln2 = nn.LayerNorm(args.nhid_tran)\n",
        "        self.ln3 = nn.LayerNorm(args.nhid_tran)\n",
        "        self.dropout1 = nn.Dropout(args.resid_pdrop)\n",
        "        self.dropout2 = nn.Dropout(args.resid_pdrop)\n",
        "        self.dropout3 = nn.Dropout(args.resid_pdrop)\n",
        "        self.attn1 = MaskedMultiheadAttention(mask=True) # self-attention \n",
        "        self.attn2 = MaskedMultiheadAttention() # tgt to src attention\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(args.nhid_tran, args.nff),\n",
        "            nn.ReLU(), \n",
        "            nn.Linear(args.nff, args.nhid_tran)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x, enc_o, enc_mask=None):\n",
        "        # WRITE YOUR CODE HERE\n",
        "        out1 = self.ln1(x)\n",
        "        out = self.attn1(out1, out1, out1)\n",
        "        out = self.dropout1(out)\n",
        "        out = out + out1\n",
        "\n",
        "        out2 = self.ln2(out)\n",
        "        out = self.attn2(out2, enc_o, enc_o, enc_mask)\n",
        "        out = self.dropout2(out)\n",
        "        out = out + out2\n",
        "\n",
        "        out3 = self.ln3(out)\n",
        "        out = self.ff(out3)\n",
        "        out = self.dropout3(out)\n",
        "\n",
        "        out = out + out3\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "RPcjs2oFxdfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len=4096):\n",
        "        super().__init__()\n",
        "        dim = args.nhid_tran\n",
        "        pos = np.arange(0, max_len)[:, None]\n",
        "        i = np.arange(0, dim // 2)\n",
        "        denom = 10000 ** (2 * i / dim)\n",
        "\n",
        "        pe = np.zeros([max_len, dim])\n",
        "        pe[:, 0::2] = np.sin(pos / denom)\n",
        "        pe[:, 1::2] = np.cos(pos / denom)\n",
        "        pe = torch.from_numpy(pe).float()\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # DO NOT MODIFY\n",
        "        return x + self.pe[:x.shape[1]]\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        # input embedding stem\n",
        "        self.tok_emb = nn.Embedding(src_ntoken, args.nhid_tran)\n",
        "        self.pos_enc = PositionalEncoding()\n",
        "        self.dropout = nn.Dropout(args.embd_pdrop)\n",
        "        # transformer\n",
        "        self.transform = nn.ModuleList([TransformerEncLayer() for _ in range(args.nlayers_transformer)])\n",
        "        # decoder head\n",
        "        self.ln_f = nn.LayerNorm(args.nhid_tran)\n",
        "        \n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # WRITE YOUR CODE HERE\n",
        "        out = self.tok_emb(x)\n",
        "        out = self.pos_enc(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        for i in self.transform:\n",
        "          out = i(out, mask)\n",
        "        out = self.ln_f(out)\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "B7cOOM7qxkhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.tok_emb = nn.Embedding(trg_ntoken, args.nhid_tran)\n",
        "        self.pos_enc = PositionalEncoding()\n",
        "        self.dropout = nn.Dropout(args.embd_pdrop)\n",
        "        self.transform = nn.ModuleList([TransformerDecLayer() for _ in range(args.nlayers_transformer)])\n",
        "        self.ln_f = nn.LayerNorm(args.nhid_tran)\n",
        "        self.lin_out = nn.Linear(args.nhid_tran, trg_ntoken)\n",
        "        self.lin_out.weight = self.tok_emb.weight\n",
        "\n",
        "\n",
        "    def forward(self, x, enc_o, enc_mask):\n",
        "        # WRITE YOUR CODE HERE\n",
        "        out = self.tok_emb(x)\n",
        "        out = self.pos_enc(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        for i in self.transform:\n",
        "          out = i(out, enc_o, enc_mask)\n",
        "        \n",
        "        out = self.ln_f(out)\n",
        "        logits = self.lin_out(out)\n",
        "        logits /= args.nhid_tran ** 0.5 # Scaling logits. Do not modify this\n",
        "        return logits"
      ],
      "metadata": {
        "id": "vW65aXDvxnvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = TransformerEncoder()\n",
        "        self.decoder = TransformerDecoder()\n",
        "        \n",
        "    def forward(self, x, y, length_x, max_len=None, teacher_forcing=True):\n",
        "        # WRITE YOUR CODE HERE\n",
        "        enc_mask = None\n",
        "        if length_x is not None:\n",
        "          T = x.shape[1]\n",
        "          B = x.shape[0]\n",
        "          lol = torch.range(0, T-1).expand(B,T).to(device)\n",
        "          lol2 = length_x.reshape(-1, 1).expand(B, T)#.to(device)\n",
        "          enc_mask = torch.zeros(B, T).to(device)\n",
        "          enc_mask[lol< lol2] = 1\n",
        "\n",
        "        enc_o = self.encoder(x, enc_mask)\n",
        "        if teacher_forcing is True or self.training is True:\n",
        "          out = self.decoder(y[:, :-1], enc_o, enc_mask)\n",
        "        else:\n",
        "          dec_input = y[:, :1]\n",
        "          for i in range(1, max_len):\n",
        "            dec_output = self.decoder(dec_input, enc_o, enc_mask)\n",
        "            dec_input = torch.concat((dec_input, dec_output[:,-1:].argmax(-1)), dim=1)\n",
        "          #dec_output = self.decoder(dec_input, enc_o, enc_mask)\n",
        "\n",
        "          out = dec_output\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "4aNeF4SAxqLW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}